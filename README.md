## DATA 512 - HOMEWORK 2 : Considering bias in Data

The goal of this assignment is to explore the concept of bias in data using Wikipedia articles. This assignment will consider articles about cities in different US states. For this assignment, you will combine a dataset of Wikipedia articles with a dataset of state populations, and use a machine learning service called ORES to estimate the quality of the articles about the cities.
An analysis is performed on how the coverage of US cities on Wikipedia and how the quality of articles about cities varies among states. The analysis consists of a series of tables that show:
* The states with the greatest and least coverage of cities on Wikipedia compared to their population.
* The states with the highest and lowest proportion of high quality articles about cities.
* A ranking of US geographic regions by articles-per-person and proportion of high quality articles.



### Data Sources:

* Data File 1: ["us_cities_by_state_SEPT.2023.csv"](https://drive.google.com/file/d/1XAydF2Cqjr5u1zs-B9p09JVliqtFYv15/view?usp=drive_link)
Description: Contains a list of Wikipedia article pages about US cities categorized by state. This data was generated by crawling the Wikipedia Category: Lists of cities in the United States by state.

* Data File 2: ["State Population Totals and Components of Change: 2020-2022"](https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html)
Description: This Excel file, available on the US Census Bureau website, provides estimated populations of all US states for the year 2022.

* Data File 3: ["Regional Division Spreadsheet"](https://drive.google.com/file/d/1uG6Pj5m3NjBbx9Xkzdtfo_Ewo0zCNK8F/view?usp=drive_link)
Description: This spreadsheet, included in the homework folder, lists the states grouped into regional and divisional agglomerations as defined by the US Census Bureau for the purpose of this analysis.

### API Information:


In the process of obtaining article quality predictions, we have utilized ORES, the Objective Revision Evaluation Service, which relies on machine learning models trained on Wikipedia articles that have undergone peer review using Wikipedia's content assessment procedures. To access ORES for making page quality predictions, we have made use of the API:Info. We referred to sample code snippets provided under the Creative Commons CC-BY license for various tasks, such as making page info requests and ORES requests. Additional resources that have proven to be helpful include the ORES Wiki, Wikipedia's Content Assessment guidelines, and Wikimedia API Documentation. In our Python notebooks, we've demonstrated how to call the Wikipedia API to retrieve article metadata and access ORES's machine learning model for article quality predictions. These resources collectively guide us in the process of evaluating and enhancing the quality of Wikipedia articles.

1. [ORES](https://www.mediawiki.org/wiki/ORES)
2. [Wikimedia]( https://www.mediawiki.org/wiki/API:Info)
3. [Notebook for making Pageinfo request]( https://drive.google.com/file/d/15UoE16s-IccCTOXREjU3xDIz07tlpyrl/view?usp=sharing)
4. [Notebook for making ORES Predictions](https://drive.google.com/file/d/17C9xsmR9U3lJeD52UTbAedlHDetwYsxs/view?usp=sharing)


### **Repository Structure**

The structure of this repository is organized as follows:

- **Input Files:**
  - `NST-EST2022-POP (3).xlsx`: A file containing population estimates for the year 2022.
  - `US States by Region - US Census Bureau - Sheet1.csv`: Provides data about US states categorized by regions as per the US Census Bureau.
  - `us_cities_by_state_SEPT.2023.csv`: Contains data related to US cities by state as of September 2023.

- **Intermediate Data Files:**
  - `title_revid.json`: A file containing JSON data of titles and revision IDs.
  - `wiki_predictions.csv`: CSV file that stores Wikipedia article quality predictions.

- **Output Files:**
  - `wp_scored_city_articles_by_state.csv`: The final output dataset containing scored city articles by state.
  - `wp_areas-no_match.txt`: A text file listing areas with no matches during data merging.

- **Source Code:**
  - The Jupyter Notebook `hcds_a2.ipynb` located in the `src` directory is used for data acquisition and analysis.

- **License:**
  - A file named `LICENSE` with an MIT License for the repository.

- **README:**
  - The `README.md` file contains instructions for reproducing the analysis, data descriptions, attributions, provenance information, and other relevant details. It also provides insights into the project's goals and learning reflections.

Feel free to navigate through the repository's content for further details on each component.

